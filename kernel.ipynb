{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "import pdb\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader, Dataset, sampler\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import albumentations as albu\n",
    "from albumentations.torch import ToTensor\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import UnetPlusPlus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 69\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLE-Mask utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/paulorzp/rle-functions-run-lenght-encode-decode\n",
    "def mask2rle(img):\n",
    "    '''\n",
    "    img: numpy array, 1 -> mask, 0 -> background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels= img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def make_mask(row_id, df):\n",
    "    '''Given a row index, return image_id and mask (256, 1600, 4) from the dataframe `df`'''\n",
    "    fname = df.iloc[row_id].name\n",
    "    labels = df.iloc[row_id][:4]\n",
    "    masks = np.zeros((256, 1600, 4), dtype=np.float32) # float32 is V.Imp\n",
    "    # 4:class 1～4 (ch:0～3)\n",
    "\n",
    "    for idx, label in enumerate(labels.values):\n",
    "        if label is not np.nan:\n",
    "            label = label.split(\" \")\n",
    "            positions = map(int, label[0::2])\n",
    "            length = map(int, label[1::2])\n",
    "            mask = np.zeros(256 * 1600, dtype=np.uint8)\n",
    "            for pos, le in zip(positions, length):\n",
    "                mask[pos:(pos + le)] = 1\n",
    "            masks[:, :, idx] = mask.reshape(256, 1600, order='F')\n",
    "    return fname, masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader + Augemation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(phase, mean, std):\n",
    "    list_transforms = []\n",
    "    if phase == \"train\":\n",
    "        list_transforms.extend(\n",
    "            [\n",
    "                albu.HorizontalFlip(p=0.5),\n",
    "                #albu.OneOf([\n",
    "                #    albu.RandomContrast(),\n",
    "                #    albu.RandomGamma(),\n",
    "                #    albu.RandomBrightness(),\n",
    "                #    ], p=0.3),\n",
    "                #albu.OneOf([\n",
    "                #    albu.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
    "                #    albu.GridDistortion(),\n",
    "                #    albu.OpticalDistortion(distort_limit=2, shift_limit=0.5),\n",
    "                #    ], p=0.3),\n",
    "               # albu.ShiftScaleRotate(),\n",
    "               # albu.Resize(img_size,img_size,always_apply=True),\n",
    "            ]\n",
    "        )\n",
    "    list_transforms.extend(\n",
    "        [\n",
    "            albu.Normalize(mean=mean, std=std, p=1),\n",
    "            ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "    list_trfms = albu.Compose(list_transforms)\n",
    "    return list_trfms\n",
    "\n",
    "class SteelDataset(Dataset):\n",
    "    def __init__(self, df, data_folder, mean, std, phase):\n",
    "        self.df = df\n",
    "        self.root = data_folder\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.phase = phase\n",
    "        self.transforms = get_transforms(phase, mean, std)\n",
    "        self.fnames = self.df.index.tolist()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id, mask = make_mask(idx, self.df)\n",
    "        image_path = os.path.join(self.root, \"train_images\",  image_id)\n",
    "        img = cv2.imread(image_path)\n",
    "        augmented = self.transforms(image=img, mask=mask)\n",
    "        img = augmented['image']\n",
    "        mask = augmented['mask'] # 1x256x1600x4\n",
    "        mask = mask[0].permute(2, 0, 1) # 1x4x256x1600\n",
    "        return img, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fnames)\n",
    "    \n",
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(SEED))\n",
    "    \n",
    "def provider(\n",
    "    data_folder,\n",
    "    df_path,\n",
    "    phase,\n",
    "    mean=None,\n",
    "    std=None,\n",
    "    batch_size=8,\n",
    "    num_workers=0,\n",
    "):\n",
    "    '''Returns dataloader for the model training'''\n",
    "    df = pd.read_csv(df_path)\n",
    "    \n",
    "    df['ImageId'], df['ClassId'] = zip(*df['ImageId_ClassId'].str.split('_'))\n",
    "    df['ClassId'] = df['ClassId'].astype(int)\n",
    "    df = df.pivot(index='ImageId',columns='ClassId',values='EncodedPixels')\n",
    "    df['defects'] = df.count(axis=1)\n",
    "    \n",
    "    train_df, val_df = train_test_split(\n",
    "        df, \n",
    "        test_size=0.2,\n",
    "        stratify=df[\"defects\"], \n",
    "        random_state=SEED)\n",
    "    \n",
    "    df = train_df if phase == \"train\" else val_df\n",
    "    image_dataset = SteelDataset(df, data_folder, mean, std, phase)\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        image_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,       \n",
    "        worker_init_fn = _init_fn\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "try:\n",
    "    from itertools import ifilterfalse\n",
    "except ImportError: \n",
    "    from itertools import filterfalse\n",
    "\n",
    "eps = 1e-6\n",
    "\n",
    "\n",
    "def dice_round(preds, trues, per_image=False):\n",
    "    preds = preds.float()\n",
    "    return soft_dice_loss(preds, trues, per_image)\n",
    "\n",
    "\n",
    "def soft_dice_loss(outputs, targets, per_image=False):\n",
    "    batch_size = outputs.size()[0]\n",
    "    eps = 1e-5\n",
    "    if not per_image:\n",
    "        batch_size = 1\n",
    "    dice_target = targets.contiguous().view(batch_size, -1).float()\n",
    "    dice_output = outputs.contiguous().view(batch_size, -1)\n",
    "    intersection = torch.sum(dice_output * dice_target, dim=1)\n",
    "    union = torch.sum(dice_output, dim=1) + torch.sum(dice_target, dim=1) + eps\n",
    "    loss = (1 - (2 * intersection + eps) / union).mean()\n",
    "    return loss\n",
    "\n",
    "def soft_focal_loss(outputs, targets, ignore_index=255, gamma=2):\n",
    "    focal_outputs = outputs.contiguous()\n",
    "    focal_targets = targets.contiguous()\n",
    "        \n",
    "    eps = 1e-8\n",
    "    non_ignored = focal_targets.view(-1) != ignore_index\n",
    "        \n",
    "    focal_targets = focal_targets.view(-1)[non_ignored].float()\n",
    "    focal_outputs = focal_outputs.contiguous().view(-1)[non_ignored]\n",
    "    focal_outputs = torch.clamp(focal_outputs, eps, 1. - eps)\n",
    "    focal_targets = torch.clamp(focal_targets, eps, 1. - eps)\n",
    "    \n",
    "    pt = (1 - focal_targets) * (1 - focal_outputs) + focal_targets * focal_outputs\n",
    "    loss = (-(1. - pt) ** gamma * torch.log(pt)).mean()\n",
    "    return loss\n",
    "\n",
    "def soft_bce_loss(outputs, targets):\n",
    "    bce_outputs = outputs.float().view(-1)\n",
    "    bce_targets = targets.float().view(-1)\n",
    "    neg_abs = - bce_outputs.abs()\n",
    "            \n",
    "    loss = (bce_outputs.clamp(min=0) - bce_outputs * bce_targets + (1 + neg_abs.exp()).log()).mean()\n",
    "    return loss\n",
    "\n",
    "def jaccard(outputs, targets, per_image=False, non_empty=False, min_pixels=5):\n",
    "    batch_size = outputs.size()[0]\n",
    "    eps = 1e-3\n",
    "    if not per_image:\n",
    "        batch_size = 1\n",
    "    dice_target = targets.contiguous().view(batch_size, -1).float()\n",
    "    dice_output = outputs.contiguous().view(batch_size, -1)\n",
    "    target_sum = torch.sum(dice_target, dim=1)\n",
    "    intersection = torch.sum(dice_output * dice_target, dim=1)\n",
    "    losses = 1 - (intersection + eps) / (torch.sum(dice_output + dice_target, dim=1) - intersection + eps)\n",
    "    if non_empty:\n",
    "        assert per_image == True\n",
    "        non_empty_images = 0\n",
    "        sum_loss = 0\n",
    "        for i in range(batch_size):\n",
    "            if target_sum[i] > min_pixels:\n",
    "                sum_loss += losses[i]\n",
    "                non_empty_images += 1\n",
    "        if non_empty_images == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return sum_loss / non_empty_images\n",
    "\n",
    "    return losses.mean()\n",
    "\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    p = len(gt_sorted)\n",
    "    gts = gt_sorted.sum()\n",
    "    intersection = gts.float() - gt_sorted.float().cumsum(0)\n",
    "    union = gts.float() + (1 - gt_sorted).float().cumsum(0)\n",
    "    jaccard = 1. - intersection / union\n",
    "    if p > 1:  # cover 1-pixel case\n",
    "        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
    "    return jaccard\n",
    "\n",
    "\n",
    "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
    "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class id\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n",
    "                    for log, lab in zip(logits, labels))\n",
    "    else:\n",
    "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lovasz_hinge_flat(logits, labels):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore: label to ignore\n",
    "    \"\"\"\n",
    "    if len(labels) == 0:\n",
    "        # only void pixels, the gradients should be 0\n",
    "        return logits.sum() * 0.\n",
    "    signs = 2. * labels.float() - 1.\n",
    "    errors = (1. - logits * Variable(signs))\n",
    "    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n",
    "    perm = perm.data\n",
    "    gt_sorted = labels[perm]\n",
    "    grad = lovasz_grad(gt_sorted)\n",
    "    loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def flatten_binary_scores(scores, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch (binary case)\n",
    "    Remove labels equal to 'ignore'\n",
    "    \"\"\"\n",
    "    scores = scores.view(-1)\n",
    "    labels = labels.view(-1)\n",
    "    if ignore is None:\n",
    "        return scores, labels\n",
    "    valid = (labels != ignore)\n",
    "    vscores = scores[valid]\n",
    "    vlabels = labels[valid]\n",
    "    return vscores, vlabels\n",
    "\n",
    "\n",
    "def lovasz_sigmoid(probas, labels, per_image=False, ignore=None):\n",
    "    \"\"\"\n",
    "    Multi-class Lovasz-Softmax loss\n",
    "      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1)\n",
    "      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n",
    "      only_present: average only on classes present in ground truth\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class labels\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        loss = mean(lovasz_sigmoid_flat(*flatten_binary_scores(prob.unsqueeze(0), lab.unsqueeze(0), ignore))\n",
    "                          for prob, lab in zip(probas, labels))\n",
    "    else:\n",
    "        loss = lovasz_sigmoid_flat(*flatten_binary_scores(probas, labels, ignore))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lovasz_sigmoid_flat(probas, labels):\n",
    "    \"\"\"\n",
    "    Multi-class Lovasz-Softmax loss\n",
    "      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n",
    "      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n",
    "      only_present: average only on classes present in ground truth\n",
    "    \"\"\"\n",
    "    fg = labels.float()\n",
    "    errors = (Variable(fg) - probas).abs()\n",
    "    errors_sorted, perm = torch.sort(errors, 0, descending=True)\n",
    "    perm = perm.data\n",
    "    fg_sorted = fg[perm]\n",
    "    loss = torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted)))\n",
    "    return loss\n",
    "\n",
    "def symmetric_lovasz(outputs, targets, ):\n",
    "    return (lovasz_hinge(outputs, targets) + lovasz_hinge(-outputs, 1 - targets)) / 2\n",
    "\n",
    "def mean(l, ignore_nan=False, empty=0):\n",
    "    \"\"\"\n",
    "    nanmean compatible with generators.\n",
    "    \"\"\"\n",
    "    l = iter(l)\n",
    "    if ignore_nan:\n",
    "        l = ifilterfalse(np.isnan, l)\n",
    "    try:\n",
    "        n = 1\n",
    "        acc = next(l)\n",
    "    except StopIteration:\n",
    "        if empty == 'raise':\n",
    "            raise ValueError('Empty mean')\n",
    "        return empty\n",
    "    for n, v in enumerate(l, 2):\n",
    "        acc += v\n",
    "    if n == 1:\n",
    "        return acc\n",
    "    return acc / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StableBCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StableBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return soft_bce_loss(input, target)\n",
    "    \n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True, per_image=False):\n",
    "        super().__init__()\n",
    "        self.size_average = size_average\n",
    "        self.register_buffer('weight', weight)\n",
    "        self.per_image = per_image\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return soft_dice_loss(input, target, per_image=self.per_image)\n",
    "    \n",
    "class JaccardLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True, per_image=False, non_empty=False, apply_sigmoid=False,\n",
    "                 min_pixels=5):\n",
    "        super().__init__()\n",
    "        self.size_average = size_average\n",
    "        self.register_buffer('weight', weight)\n",
    "        self.per_image = per_image\n",
    "        self.non_empty = non_empty\n",
    "        self.apply_sigmoid = apply_sigmoid\n",
    "        self.min_pixels = min_pixels\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if self.apply_sigmoid:\n",
    "            input = torch.sigmoid(input)\n",
    "        return jaccard(input, target, per_image=self.per_image, non_empty=self.non_empty, min_pixels=self.min_pixels)\n",
    "    \n",
    "class LovaszLoss(nn.Module):\n",
    "    def __init__(self, ignore_index=255, per_image=True):\n",
    "        super().__init__()\n",
    "        self.ignore_index = ignore_index\n",
    "        self.per_image = per_image\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        outputs = outputs.contiguous()\n",
    "        targets = targets.contiguous()\n",
    "        return symmetric_lovasz(outputs, targets)\n",
    "\n",
    "class LovaszLossSigmoid(nn.Module):\n",
    "    def __init__(self, ignore_index=255, per_image=True):\n",
    "        super().__init__()\n",
    "        self.ignore_index = ignore_index\n",
    "        self.per_image = per_image\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        outputs = outputs.contiguous()\n",
    "        targets = targets.contiguous()\n",
    "        return lovasz_sigmoid(outputs, targets, per_image=self.per_image, ignore=self.ignore_index)\n",
    "\n",
    "class FocalLoss2d(nn.Module):\n",
    "    def __init__(self, gamma=2, ignore_index=255):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, input, targets):\n",
    "        return soft_focal_loss(input, targets, self.ignore_index, self.gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss should be reimplemented to calculate losses for each class\n",
    "(num_neg_in_validation * loss_negative + num_pos1_in_validation * loss1_pos ...) / total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComboLoss(nn.Module):\n",
    "    def __init__(self, weights, per_image=False, channel_weights=[1, 1, 1, 1], channel_losses=None):\n",
    "        super().__init__()\n",
    "        self.weights = weights\n",
    "        self.bce = StableBCELoss()\n",
    "        self.dice = DiceLoss(per_image=False)\n",
    "        self.jaccard = JaccardLoss(per_image=False)\n",
    "        self.lovasz = LovaszLoss(per_image=per_image)\n",
    "        self.lovasz_sigmoid = LovaszLossSigmoid(per_image=per_image)\n",
    "        self.focal = FocalLoss2d()\n",
    "        self.mapping = {'bce': self.bce,\n",
    "                        'dice': self.dice,\n",
    "                        'focal': self.focal,\n",
    "                        'jaccard': self.jaccard,\n",
    "                        'lovasz': self.lovasz,\n",
    "                        'lovasz_sigmoid': self.lovasz_sigmoid}\n",
    "        self.expect_sigmoid = {'dice', 'focal', 'jaccard', 'lovasz_sigmoid'}\n",
    "        self.per_channel = {'dice', 'jaccard', 'lovasz_sigmoid'}\n",
    "        self.values = {}\n",
    "        self.channel_weights = channel_weights\n",
    "        self.channel_losses = channel_losses\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        loss = 0\n",
    "        weights = self.weights\n",
    "        sigmoid_input = torch.sigmoid(outputs)\n",
    "        \n",
    "        for k, v in weights.items():\n",
    "            if not v:\n",
    "                continue\n",
    "            val = 0\n",
    "            if k in self.per_channel:\n",
    "                channels = targets.size(1)\n",
    "                for c in range(channels):\n",
    "                    if not self.channel_losses or k in self.channel_losses[c]:\n",
    "                        val += self.channel_weights[c] * self.mapping[k](sigmoid_input[:, c, ...] if k in self.expect_sigmoid else outputs[:, c, ...],\n",
    "                                               targets[:, c, ...])\n",
    "\n",
    "            else:\n",
    "                val = self.mapping[k](sigmoid_input if k in self.expect_sigmoid else outputs, targets)\n",
    "\n",
    "            self.values[k] = val\n",
    "            loss += self.weights[k] * val\n",
    "        return loss.clamp(min=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ious(pred, label, classes, ignore_index=255, only_present=True):\n",
    "    '''computes iou for one ground truth mask and predicted mask'''\n",
    "    pred[label == ignore_index] = 0\n",
    "    ious = []\n",
    "    for c in classes:\n",
    "        label_c = label == c\n",
    "        if only_present and np.sum(label_c) == 0:\n",
    "            ious.append(np.nan)\n",
    "            continue\n",
    "        pred_c = pred == c\n",
    "        intersection = np.logical_and(pred_c, label_c).sum()\n",
    "        union = np.logical_or(pred_c, label_c).sum()\n",
    "        if union != 0:\n",
    "            ious.append(intersection / union)\n",
    "    return ious if ious else [1]\n",
    "\n",
    "def compute_iou_batch(outputs, labels, classes=None):\n",
    "    '''computes mean iou for a batch of ground truth masks and predicted masks'''\n",
    "    ious = []\n",
    "    preds = np.copy(outputs) # copy is imp\n",
    "    labels = np.array(labels) # tensor to np\n",
    "    for pred, label in zip(preds, labels):\n",
    "        ious.append(np.nanmean(compute_ious(pred, label, classes)))\n",
    "    iou = np.nanmean(ious)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A dice loss from the original kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(probability, truth, threshold=0.5, reduction='none'):\n",
    "    '''Calculates dice of positive and negative images seperately'''\n",
    "    '''probability and truth must be torch tensors'''\n",
    "    batch_size = len(truth)\n",
    "    with torch.no_grad():\n",
    "        probability = probability.view(batch_size, -1)\n",
    "        truth = truth.view(batch_size, -1)\n",
    "        assert(probability.shape == truth.shape)\n",
    "\n",
    "        p = (probability > threshold).float()\n",
    "        t = (truth > 0.5).float()\n",
    "\n",
    "        t_sum = t.sum(-1)\n",
    "        p_sum = p.sum(-1)\n",
    "        neg_index = torch.nonzero(t_sum == 0)\n",
    "        pos_index = torch.nonzero(t_sum >= 1)\n",
    "\n",
    "        dice_neg = (p_sum == 0).float()\n",
    "        dice_pos = 2 * (p*t).sum(-1)/((p+t).sum(-1))\n",
    "\n",
    "        dice_neg = dice_neg[neg_index]\n",
    "        dice_pos = dice_pos[pos_index]\n",
    "        dice = torch.cat([dice_pos, dice_neg])\n",
    "\n",
    "        dice_neg = np.nan_to_num(dice_neg.mean().item(), 0)\n",
    "        dice_pos = np.nan_to_num(dice_pos.mean().item(), 0)\n",
    "        dice = dice.mean().item()\n",
    "\n",
    "        num_neg = len(neg_index)\n",
    "        num_pos = len(pos_index)\n",
    "\n",
    "    return dice, dice_neg, dice_pos, num_neg, num_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, threshold):\n",
    "    '''X is sigmoid output of the model'''\n",
    "    X_p = np.copy(X)\n",
    "    preds = (X_p > threshold).astype('uint8')\n",
    "    return preds\n",
    "\n",
    "class Meter:\n",
    "    '''A meter to keep track of iou and dice scores throughout an epoch'''\n",
    "    def __init__(self, phase, epoch):\n",
    "        self.base_threshold = 0.5 # <<<<<<<<<<< here's the threshold\n",
    "        self.base_dice_scores = []\n",
    "        self.dice_neg_scores = []\n",
    "        self.dice_pos_scores = []\n",
    "        self.iou_scores = []\n",
    "        self.base_bce_scores = []\n",
    "        self.base_focal_scores = []\n",
    "        self.base_dice2_scores = []\n",
    "\n",
    "    def update(self, targets, outputs):\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        dice, dice_neg, dice_pos, _, _ = metric(probs, targets, self.base_threshold)\n",
    "        self.base_dice_scores.append(dice)\n",
    "        self.dice_pos_scores.append(dice_pos)\n",
    "        self.dice_neg_scores.append(dice_neg)\n",
    "        \n",
    "        preds = predict(probs, self.base_threshold)\n",
    "        iou = compute_iou_batch(preds, targets, classes=[1])\n",
    "        self.iou_scores.append(iou)\n",
    "        \n",
    "        bce = soft_bce_loss(probs, targets)\n",
    "        self.base_bce_scores.append(bce)\n",
    "        \n",
    "        focal = soft_focal_loss(probs, targets)\n",
    "        self.base_focal_scores.append(focal)\n",
    "        \n",
    "        dice2 = soft_dice_loss(probs, targets)\n",
    "        self.base_dice2_scores.append(dice2)\n",
    "\n",
    "    def get_metrics(self):\n",
    "        dice = np.mean(self.base_dice_scores)\n",
    "        dice_neg = np.mean(self.dice_neg_scores)\n",
    "        dice_pos = np.mean(self.dice_pos_scores)\n",
    "        dices = [dice, dice_neg, dice_pos]\n",
    "    \n",
    "        iou = np.nanmean(self.iou_scores) \n",
    "        \n",
    "        bce = np.mean(self.base_bce_scores)\n",
    "        focal = np.mean(self.base_focal_scores)    \n",
    "        dice2 = np.mean(self.base_dice2_scores)\n",
    "        \n",
    "        return dices, bce, focal, iou, dice2\n",
    "\n",
    "def epoch_log(phase, epoch, epoch_loss, meter, start):\n",
    "    '''logging the metrics at the end of an epoch'''\n",
    "    \n",
    "    dices, bce, focal, iou, dice2 = meter.get_metrics()\n",
    "    dice, dice_neg, dice_pos = dices\n",
    "    \n",
    "    print(\"Loss: %0.4f | IoU: %0.4f | dice: %0.4f | dice_neg: %0.4f | dice_pos: %0.4f | bce: %0.4f | focal: %0.4f | dice2: %0.4f\" % (epoch_loss, iou, dice, dice_neg, dice_pos, bce, focal, dice2))\n",
    "    \n",
    "    return dice, bce, focal, iou\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UnetPlusPlus(\"resnet18\", encoder_weights=\"imagenet\", classes=4, activation='sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    '''This class takes care of training and validation of our model'''\n",
    "    def __init__(self, model):\n",
    "        self.num_workers = 0\n",
    "        self.batch_size = {\"train\": 4, \"val\": 4}\n",
    "        self.accumulation_steps = 32 // self.batch_size['train']\n",
    "        self.lr = 5e-4\n",
    "        self.num_epochs = 2\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.phases = [\"train\", \"val\"]\n",
    "        self.device = torch.device(\"cuda:0\")\n",
    "        torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "        self.net = model\n",
    "        self.criterion = ComboLoss(weights={'bce':1, 'dice':1,'focal':1})\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"min\", patience=3, verbose=True)\n",
    "        self.net = self.net.to(self.device)\n",
    "        cudnn.benchmark = True\n",
    "        self.dataloaders = {\n",
    "            phase: provider(\n",
    "                data_folder=data_folder,\n",
    "                df_path=train_df_path,\n",
    "                phase=phase,\n",
    "                mean=(0.485, 0.456, 0.406), ###????\n",
    "                std=(0.229, 0.224, 0.225),  ###????\n",
    "                batch_size=self.batch_size[phase],\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "            for phase in self.phases\n",
    "        }\n",
    "        self.losses = {phase: [] for phase in self.phases}\n",
    "        self.iou_scores = {phase: [] for phase in self.phases}\n",
    "        self.dice_scores = {phase: [] for phase in self.phases}\n",
    "        self.bce_scores = {phase: [] for phase in self.phases}\n",
    "        self.focal_scores = {phase: [] for phase in self.phases}\n",
    "        \n",
    "    def forward(self, images, targets):\n",
    "        images = images.to(self.device)\n",
    "        masks = targets.to(self.device)\n",
    "        outputs = self.net(images)\n",
    "        loss = self.criterion(outputs, masks)\n",
    "        return loss, outputs\n",
    "\n",
    "    def iterate(self, epoch, phase):\n",
    "        meter = Meter(phase, epoch)\n",
    "        \n",
    "        start = time.strftime(\"%H:%M:%S\")\n",
    "        print(f\"Starting epoch: {epoch} | phase: {phase} | ⏰: {start}\")\n",
    "        \n",
    "        batch_size = self.batch_size[phase]\n",
    "        self.net.train(phase == \"train\")\n",
    "        dataloader = self.dataloaders[phase]\n",
    "        running_loss = 0.0\n",
    "        total_batches = len(dataloader)\n",
    "        self.optimizer.zero_grad()\n",
    "    \n",
    "        for itr, batch in enumerate(dataloader):\n",
    "            images, targets = batch\n",
    "            loss, outputs = self.forward(images, targets)\n",
    "            loss = loss / self.accumulation_steps\n",
    "            if phase == \"train\":\n",
    "                loss.backward()\n",
    "                if (itr + 1 ) % self.accumulation_steps == 0:\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "            running_loss += loss.item()\n",
    "            outputs = outputs.detach().cpu()\n",
    "            meter.update(targets, outputs)\n",
    "\n",
    "        epoch_loss = (running_loss * self.accumulation_steps) / total_batches\n",
    "        \n",
    "        dice, bce, focal, iou = epoch_log(phase, epoch, epoch_loss, meter, start)   \n",
    "        self.losses[phase].append(epoch_loss)\n",
    "        self.dice_scores[phase].append(dice)\n",
    "        self.bce_scores[phase].append(bce)\n",
    "        self.focal_scores[phase].append(focal)\n",
    "        self.iou_scores[phase].append(iou)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return epoch_loss\n",
    "\n",
    "    def start(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.iterate(epoch, \"train\")\n",
    "            state = {\n",
    "                \"epoch\": epoch,\n",
    "                \"best_loss\": self.best_loss,\n",
    "                \"state_dict\": self.net.state_dict(),\n",
    "                \"optimizer\": self.optimizer.state_dict(),\n",
    "            }\n",
    "            \n",
    "            #to prevent GPU memory from overflowing on validation\n",
    "            with torch.no_grad():\n",
    "                val_loss = self.iterate(epoch, \"val\")\n",
    "                self.scheduler.step(val_loss)\n",
    "                \n",
    "            if val_loss < self.best_loss:\n",
    "                print(\"******** New optimal found, saving state ********\")\n",
    "                state[\"best_loss\"] = self.best_loss = val_loss\n",
    "                torch.save(state, \"./model.pth\")\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_path = './data/sample_submission.csv'\n",
    "train_df_path = './data/train.csv'\n",
    "data_folder = \"./data/\"\n",
    "test_data_folder = \"./data/test_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: train | ⏰: 08:59:53\n",
      "Loss: 4.1164 | IoU: 0.2511 | dice: 0.2087 | dice_neg: 0.0541 | dice_pos: 0.3252 | bce: 0.7988 | focal: 0.0411 | dice2: 0.9571\n",
      "Starting epoch: 0 | phase: val | ⏰: 09:33:20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-05f46443fc03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel_trainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_trainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-b289f24b05ca>\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;31m#to prevent GPU memory from overflowing on validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m                 \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"val\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-b289f24b05ca>\u001b[0m in \u001b[0;36miterate\u001b[1;34m(self, epoch, phase)\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mmeter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrunning_loss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccumulation_steps\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtotal_batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-2a21c9ad20ae>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, targets, outputs)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miou_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miou\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mbce\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoft_bce_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_bce_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-0b11f7a5ba7b>\u001b[0m in \u001b[0;36msoft_bce_loss\u001b[1;34m(outputs, targets)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mneg_abs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbce_outputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbce_outputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbce_outputs\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbce_targets\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mneg_abs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_trainer = Trainer(model)\n",
    "model_trainer.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT TRAINING\n",
    "losses = model_trainer.losses\n",
    "dice_scores = model_trainer.dice_scores # overall dice\n",
    "iou_scores = model_trainer.iou_scores\n",
    "\n",
    "def plot(scores, name):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(range(len(scores[\"train\"])), scores[\"train\"], label=f'train {name}')\n",
    "    plt.plot(range(len(scores[\"train\"])), scores[\"val\"], label=f'val {name}')\n",
    "    plt.title(f'{name} plot'); plt.xlabel('Epoch'); plt.ylabel(f'{name}');\n",
    "    plt.legend(); \n",
    "    plt.show()\n",
    "\n",
    "plot(losses, \"BCE loss\")\n",
    "plot(dice_scores, \"Dice score\")\n",
    "plot(iou_scores, \"IoU score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
